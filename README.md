# Introduction
## Adding a new connector

1. First, find the latest tag of a connector. To do so, find your connector in airbyte repository [here](https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors) and look at the `Dockerfile` of the connector, there will be a `io.airbyte.version` LABEL at the bottom of the `Dockerfile`, use the value of this label exactly as it is.

2. Then, run `new-connector.sh` script with two arguments: the connector name and the version tag. Example: `./new-connector.sh freshdesk 0.1.0`. This will create a new directory `airbyte-integrations/connectors/source-freshdesk`.

3. Now you can build a local docker container of this connector so you can run it using `flowctl` and `flowctl-go` to interact with it. To build the connector, run:
```
./local-build.sh source-freshdesk
```

4. Once you have the connector built, there are a few things that will need to be checked and updated as necessary:

#### Checking the rendering of the form on the UI

To check how the form for this connector would render in the UI, you will need to:
1. Add it to the `connectors` table on supabase
2. Visit the [form test page](https://dashboard.estuary.dev/test/jsonforms) and choose your connector from the list at the top
3. Take the connector config schema and paste it there, to do so, run the following command and copy the output:

```
flowctl-go api spec --image ghcr.io/estuary/source-freshdesk:local | jq '.configSchema'
```

Depending on how the form renders, you may want to patch certain things on the config specification schema, to do so you can use the `spec.patch.json` file. See [Patching](#patching) section below for more details.

#### Checking the schema of different connector streams (collection schemas)

It is important to check the schema of different collections of the connector. To do so, we will need to first discover the connector and then run it to ensure we can actually capture documents without document schema violations.

To discover the connector, run the following command:

```
flowctl raw discover ghcr.io/estuary/source-freshdesk:local
```

After running this command once, you should be able to find some files added to your environment, namely, `source-freshdesk.flow.yaml` and `acmeCo/flow.yaml` files. You should now open the `acmeCo/flow.yaml` file and fill in the `config` section with appropriate values that would allow you to capture data with this connector.

Once you have the configuration filled in, you can run the discover command once more to actually discover the collections of this connector:

```
flowctl raw discover ghcr.io/estuary/source-freshdesk:local
```

If the process has been successful, you should see more files under `acmeCo/` for schema of different collections. Now that we have the capture discovered, we can try running it for a while to see if we hit any issues with schema violations, etc.

To run the connector, use the `flowctl raw capture` command, with the `*.flow.yaml` file generated from the previous steps as its argument:

```
flowctl raw capture source-freshdesk.flow.yaml
```

This command will attempt to run the connector for some time and output documents to you. Check the documents and make sure there are no errors.

If there are errors about schema violations, we need to patch the stream schemas to arrive at a schema that is not violated by the documents anymore. See [Patching](#patching) section below for more.

## Patching

These files can be placed in the root directory of the connector
and copied in Dockerfile. The following files are supported:

1. `spec.patch.json`: to patch the connector's endpoint_spec, the patch is applied as specified by [RFC7396 JSON Merge](https://www.rfc-editor.org/rfc/rfc7396.txt)
2. `spec.map.json`: to map fields from endpoint_spec. Keys and values are JSON pointers. Each key: value in this file is processed by moving whatever is at the value pointer to the key pointer
3. `oauth2.patch.json`: to patch the connector's oauth2 spec. This patch overrides the connector's oauth2 spec
4. `documentation_url.patch.json`: to patch the connector's
   documentation_url. Expects a single key `documentation_url` with a string value
5. `run_interval_minutes.json`: to set a run interval for connector. The value of this file should be a number that indicates how many minutes should runs of the connector be spaced. For example, a value of 30 means that the connector will not run successfully more frequently than once every 30 minutes (if the connector errors, it will be restarted immediately).
5. `streams/<stream-name>.patch.json`: to patch a specific stream's document schema
6. `streams/<stream-name>.pk.json`: to patch a specific stream's primary key, expects an array of strings
7. `streams/<stream-name>.normalize.json`: to apply data normalization functions to specific fields of documents generated by a capture stream. Normalizations are provided as a list (JSON array) of objects with keys `pointer` having a value of the pointer to the document field that the normalization will apply to, and `normalization` having a value of the name of the normalization function to apply to the field at that pointer. Normalization function names should provided as `snake_case`; see `airbyte_to_flow/src/interceptors/normalize.rs` for the supported normalization functions.

After updating patch files, make sure you build the connector again before running commands to test it, a re-build is necessary to include your patch changes.

# CI/CD

To build images for this new connector, you need to add this connector name
   to `.github/workflows/connectors.yml`,
   `jobs.build_connectors.strategy.matrix.connector` is the place to add the new
   connector.

## Updating an existing connector (no code)

To update a connector, open the `Dockerfile` of that connector, and in the `FROM airbyte/source-${NAME}:${VERSION}` line, replace `VERSION` with the latest tag of the connector.

# Advanced

## Local Flow testing
To test the connector in your local flow setup, you can push the connector with
a personal tag. First you need to login docker to `ghcr.io`, to do this you need
to create a [github personal
access
token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)
with access to push to our container registry. Once you have this token, you can
login using the command below:

```
echo '<YOUR-TOKEN>' | docker login ghcr.io -u <YOUR-GITHUB-USERNAME> --password-stdin
```

Then you can push the connector by passing `-p` to `local.build.sh`. This
command will push the docker container with a tag unique to your codespace which will
be printed at the end.

```
./local-build.sh -p source-intercom
```

## Updating an existing connector (connectors with code)

The `pull-connector.sh` script can update existing connectors as well. You can
just run:

```
./pull-connector.sh source-hubspot
```

The script will take you through a diff of the latest version from airbyte and
our local version, and will ask you about each file whether we should keep the
local file or take the file from upstream. A rule of thumb is that we want to
pull in code changes, but we usually keep our local version of `Dockerfile` and
`.dockerignore`. Don't forget to bump the version in `Dockerfile` if the changes
we are pulling from upstream are backward-incompatible.

## airbyte-to-flow

See the README file in `airbyte-to-flow` directory.
