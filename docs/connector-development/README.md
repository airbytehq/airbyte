# Connector Development

Airbyte supports two types of connectors: Sources and Destinations. A connector takes the form of a Docker image which follows the [Airbyte specification](../understanding-airbyte/airbyte-protocol.md).

To build a new connector in Java or Python, we provide templates so you don't need to start everything from scratch.

**Note: you are not required to maintain the connectors you create.** The goal is that the Airbyte core team and the community help maintain the connector.

## Low-code Connector-Development Framework

You can use the [low-code framework](config-based/low-code-cdk-overview.md) to build source connectors for REST APIs via a [connector builder UI](config-based/connector-builder-ui.md) or by modifying boilerplate YAML files.

## Python Connector-Development Kit \(CDK\)

You can build a connector very quickly in Python with the [Airbyte CDK](cdk-python/), which generates 75% of the code required for you.

## C#/.NET Connector-Development Kit \(CDK\)

You can build a connector very quickly in C# .NET with the [Airbyte Dotnet CDK](cdk-dotnet/), which generates 75% of the code required for you.

## TS/JS Connector-Development Kit \(Faros AI Airbyte CDK\)

You can build a connector in TypeScript/JavaScript with the [Faros AI CDK](https://github.com/airbytehq/airbyte/tree/01b905a38385ca514c2d9c07cc44a8f9a48ce762/docs/connector-development/cdk-faros-js/README.md), which generates and boostraps most of the code required for HTTP Airbyte sources.

## The Airbyte specification

Before building a new connector, review [Airbyte's data protocol specification](../understanding-airbyte/airbyte-protocol.md).

## Adding a new connector

### Requirements

To add a new connector you need to:

1. Implement & Package your connector in an Airbyte Protocol compliant Docker image
2. Add integration tests for your connector. At a minimum, all connectors must pass [Airbyte's standard test suite](testing-connectors/), but you can also add your own tests. 
3. Document how to build & test your connector
4. Publish the Docker image containing the connector

Each requirement has a subsection below.

### 1. Implement & package the connector

If you are building a connector in any of the following languages/frameworks, then you're in luck! We provide autogenerated templates to get you started quickly:

#### Sources

* **Python Source Connector**
* [**Singer**](https://singer.io)**-based Python Source Connector**. [Singer.io](https://singer.io/) is an open source framework with a large community and many available connectors \(known as taps & targets\). To build an Airbyte connector from a Singer tap, wrap the tap in a thin Python package to make it Airbyte Protocol-compatible. See the [Github Connector](https://github.com/airbytehq/airbyte/tree/master/airbyte-integrations/connectors/source-github-singer) for an example of an Airbyte Connector implemented on top of a Singer tap.
* **Generic Connector**: This template provides a basic starting point for any language.

#### Destinations

* **Java Destination Connector**
* **Python Destination Connector**

#### Creating a connector from a template

Run the interactive generator:

```text
cd airbyte-integrations/connector-templates/generator
./generate.sh
```

and choose the relevant template by using the arrow keys. This will generate a new connector in the `airbyte-integrations/connectors/<your-connector>` directory.

Search the generated directory for "TODO"s and follow them to implement your connector. For more detailed walkthroughs and instructions, follow the relevant tutorial:

* [Speedrun: Building a HTTP source with the CDK](tutorials/cdk-speedrun.md)
* [Building a HTTP source with the CDK](tutorials/cdk-tutorial-python-http/getting-started.md)
* [Building a Python source](tutorials/building-a-python-source.md) 
* [Building a Python destination](tutorials/building-a-python-destination.md)
* [Building a Java destination](tutorials/building-a-java-destination.md)

As you implement your connector, make sure to review the [Best Practices for Connector Development](best-practices.md) guide. Following best practices is not a requirement for merging your contribution to Airbyte, but it certainly doesn't hurt ;\)

### 2. Integration tests

At a minimum, your connector must implement the acceptance tests described in [Testing Connectors](testing-connectors/)

**Note: Acceptance tests are not yet available for Python destination connectors. Coming** [**soon**](https://github.com/airbytehq/airbyte/issues/4698)**!**

### 3. Document building & testing your connector

If you're writing in Python or Java, skip this section -- it is provided automatically.

If you're writing in another language, please document the commands needed to:

1. Build your connector docker image \(usually this is just `docker build .` but let us know if there are necessary flags, gotchas, etc..\) 
2. Run any unit or integration tests _in a Docker image_.

Your integration and unit tests must be runnable entirely within a Docker image. This is important to guarantee consistent build environments.

When you submit a PR to Airbyte with your connector, the reviewer will use the commands you provide to integrate your connector into Airbyte's build system as follows:

1. `:airbyte-integrations:connectors:source-<name>:build` should run unit tests and build the integration's Docker image 
2. `:airbyte-integrations:connectors:source-<name>:integrationTest` should run integration tests including Airbyte's Standard test suite.

### 4. Publish the connector

Typically this will be handled as part of code review by an Airbyter. There is a section below on what steps are needed for publishing a connector and will mostly be used by Airbyte employees publishing the connector.

## Updating an existing connector

The steps for updating an existing connector are the same as for building a new connector minus the need to use the autogenerator to create a new connector. Therefore the steps are:

1. Iterate on the connector to make the needed changes
2. Run tests
3. Add any needed docs updates
4. Create a PR to get the connector published

## Adding normalization to a connector

In order to enable normalization for a destination connector, you'll need to set some fields on the destination definitions entry for the connector. This is done in the `airbyte-config/init/src/main/resources/seed/destination_definitions.yaml` file.

### New connectors
If you're adding normalization to a new connector, you'll need to first add a destination definitions entry:
1. Add a new connector definition in `airbyte-config/init/src/main/resources/seed/destination_definitions.yaml`. You can copy an existing entry and modify it to match your connector, generating a new UUIDv4 for the `destinationDefinitionId`.
2. Run the command `./gradlew :airbyte-config:init:processResources` to generate the seed spec yaml files, and commit the changes to the PR. See [this readme](https://github.com/airbytehq/airbyte/tree/master/airbyte-config/specs) for more information.

### Add normalization fields

Once you have a destination definitions entry, you'll need to add a `normaliationConfig` field to enable normalization.

Here's an example of normalization fields being set to enable normalization for the Postgres destination:

```yaml
normalizationConfig:
    normalizationRepository: airbyte/normalization
    normalizationTag: 0.2.25
    normalizationIntegrationType: postgres
```

For more information about what these fields mean, see the [NormalizationDestinationDefinitionConfig](https://github.com/airbytehq/airbyte/blob/master/airbyte-config/config-models/src/main/resources/types/NormalizationDestinationDefinitionConfig.yaml) schema.

The presence of these fields will enable normalization for the connector, and determine which docker image will run.

## Publishing a connector

Once you've finished iterating on the changes to a connector as specified in its `README.md`, follow these instructions to ship the new version of the connector with Airbyte out of the box.

1. Bump the version in the `Dockerfile` of the connector \(`LABEL io.airbyte.version=X.X.X`\). 
2. Submit a PR containing the changes you made.
3. One of Airbyte maintainers will review the change and publish the new version of the connector to Docker hub. Triggering tests and publishing connectors can be done by leaving a comment on the PR with the following format \(the PR must be from the Airbyte repo, not a fork\):

   ```text
   # to run integration tests for the connector
   # Example: /test connector=connectors/source-hubspot
   /test connector=(connectors|bases)/<connector_name> 

   # to run integration tests, publish the connector, and use the updated connector version in our config/metadata files
   # Example: /publish connector=connectors/source-hubspot
   /publish connector=(connectors|bases)/<connector_name>
   ```
   
4. OPTIONAL: Necessary if this is a new connector, or the automated connector version bump fails
   * Update/Add the connector definition in the Airbyte connector index to use the new version:
        * `airbyte-config/init/src/main/resources/seed/source_definitions.yaml` if it is a source
        * `airbyte-config/init/src/main/resources/seed/destination_definitions.yaml` if it is a destination.
   
   * Then run the command `./gradlew :airbyte-config:init:processResources` to generate the seed spec yaml files, and commit the changes to the PR. See [this readme](https://github.com/airbytehq/airbyte/tree/a534bb2a8f29b20e3cc7c52fef1bc3c34783695d/airbyte-config/specs) for more information.
   
5. If the `README.md` file of the connector contains a `Changelog` section, add the new version and relevant release information to the table in the section.
6. The new version of the connector is now available for everyone who uses it. Thank you!

### The /publish command

Publishing a connector can be done using the `/publish` command as outlined in the above section. The command runs a [github workflow](https://github.com/airbytehq/airbyte/actions/workflows/publish-command.yml), and has the following configurable parameters:
* **connector** - Required. This tells the workflow which connector to publish. e.g. `connector=connectors/source-amazon-ads`. This can also be a comma-separated list of many connectors, e.g. `connector=connectors/source-s3,connectors/destination-postgres,connectors/source-facebook-marketing`. See the parallel flag below if publishing multiple connectors.
* **repo** - Defaults to the main airbyte repo. Set this when building connectors from forked repos. e.g. `repo=userfork/airbyte`
* **gitref** - Defaults to the branch of the PR where the /publish command is run as a comment. If running manually, set this to your branch where you made changes e.g. `gitref=george/s3-update`
* **comment-id** - This is automatically filled if you run /publish from a comment and enables the workflow to write back success/fail logs to the git comment.
* **auto-bump-version** - Defaults to true, automates the post-publish process of bumping the connector's version in the yaml seed definitions and generating spec.
* **parallel** - Defaults to false. If set to true, a pool of runner agents will be spun up to allow publishing multiple connectors in parallel. Only switch this to true if publishing multiple connectors at once to avoid wasting $$$.

## Using credentials in CI

In order to run integration tests in CI, you'll often need to inject credentials into CI. There are a few steps for doing this:
1. **Place the credentials into Google Secret Manager(GSM)**: Airbyte uses a project 'Google Secret Manager' service as the source of truth for all CI secrets. Place the credentials **exactly as they should be used by the connector** into a GSM secret [here](https://console.cloud.google.com/security/secret-manager?referrer=search&orgonly=true&project=dataline-integration-testing&supportedpurview=organizationId) i.e.: it should basically be a copy paste of the `config.json` passed into a connector via the `--config` flag. We use the following naming pattern: `SECRET_<capital source OR destination name>_CREDS` e.g: `SECRET_SOURCE-S3_CREDS` or `SECRET_DESTINATION-SNOWFLAKE_CREDS`.
2. **Add the GSM secret's labels**:
    * `connector` (required) -- unique connector's name or set of connectors' names with '_' as delimiter i.e.: `connector=source-s3`, `connector=destination-snowflake`
    * `filename` (optional) -- custom target secret file. Unfortunately Google doesn't use '.' into labels' values and so Airbyte CI scripts will add '.json' to the end automatically. By default secrets will be saved to `./secrets/config.json` i.e: `filename=config_auth` => `secrets/config_auth.json`
3. **Save a necessary JSON value** [Example](https://user-images.githubusercontent.com/11213273/146040653-4a76c371-a00e-41fe-8300-cbd411f10b2e.png).
4. That should be it.

#### Access CI secrets on GSM
Access to GSM storage is limited to Airbyte employees. To give an employee permissions to the project:
1. Go to the permissions' [page](https://console.cloud.google.com/iam-admin/iam?project=dataline-integration-testing)
2. Add a new principal to `dataline-integration-testing`:
- input their login email
- select the role `Development_CI_Secrets`
3. Save


