# S3

## Overview

This destination writes data to S3 bucket.

The Airbyte S3 destination allows you to sync data to AWS S3. Each stream is written to its own directory under the bucket.

## Sync Mode

| Feature | Support | Notes |
| :--- | :---: | :--- |
| Full Refresh Sync | ✅ | Warning: this mode deletes all previously synced data in the configured bucket path. |
| Incremental - Append Sync | ✅ | |
| Namespaces | ❌ | Setting a specific bucket path is equivalent to having separate namespaces. |

## Configuration

| Parameter | Type | Notes |
| :--- | :---: | :--- |
| S3 Bucket Name | string | Name of the bucket to sync data into. |
| S3 Bucket Path | string | Subdirectory under the above bucket to sync the data into. |
| S3 Region | string | See [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions) for all region codes. |
| Access Key ID | string | AWS credential. |
| Secret Access Key | string | AWS credential. |
| Format | object | Format specific configuration. See below for details. |

⚠️ Please note that under "Full Refresh Sync" mode, data in the configured bucket and path will be wiped out before each sync. We recommend you to provision a dedicated S3 resource for this sync to prevent unexpected data deletion from misconfiguration. ⚠️

The full path of the output data is:

```
<bucket-name>/<sorce-namespace-if-exists>/<stream-name>/<upload-date>-<upload-mills>-<partition-id>.<format-extension>
```

For example:

```
testing_bucket/data_output_path/public/users/2021_01_01_1609541171643_0.csv
↑              ↑                ↑      ↑     ↑          ↑             ↑ ↑
|              |                |      |     |          |             | format extension
|              |                |      |     |          |             partition id
|              |                |      |     |          upload time in millis
|              |                |      |     upload date in YYYY-MM-DD
|              |                |      stream name
|              |                source namespace (if it exists)
|              bucket path
bucket name
```

Please note that the stream name may contain a prefix, if it is configured on the connection.

The rationales behind this naming pattern are:
1. Each stream has its own directory.
2. The data output files can be sorted by upload time.
3. The upload time is composed of a date part and millis part so that it is both readable and unique.

Currently, each data sync will only create one file per stream. In the future, the output file can be partitioned by size. Each partition is identifiable by the partition ID, which is always 0 for now.

## Output Schema

Currently, this connector only writes data as CSV files. More formats (e.g. Apache Parquet) will be supported in the future.

### CSV

Each stream will be outputted to its dedicated directory according to the configuration. The complete datastore of each stream includes all the CSV files under that directory. You can think of the directory as equivalent of a Table in the database world.
- Under "Full Refresh Sync" mode, old CSV files will be purged before new files are created.
- Under "Incremental - Append Sync" mode, new CSV files will be added that only contain the new data.

Each CSV file includes at least two Airbyte metadata columns. Depending on the `flattening` config, the data may reside in one column (`_airbyte_data`) when there is no flattening, or multiple columns with root level flattening.

| Column | Condition | Description |
| :--- | :--- | :--- |
| `_airbyte_ab_id` | Always exists | A uuid assigned by Airbyte to each processed record. |
| `_airbyte_emitted_at` | Always exists. | A timestamp representing when the event was pulled from the data source. |
| `_airbyte_data` | When no flattening is needed, all data reside under this column as a json blob. |
| root level fields| When root level flattening is selected, the root level fields are expanded. |

 For example, given the following json object from a source:

```json
{
  "user_id": 123,
  "name": {
    "first": "John",
    "last": "Doe"
  }
}
```

With no flattening, the output CSV is:

| `_airbyte_ab_id` | `_airbyte_emitted_at` | `_airbyte_data` |
| :--- | :--- | :--- |
| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | `{ "user_id":123, "name": { "first": "John", "last": "Doe" } }` |

With root level flattening, the output CSV is:

| `_airbyte_ab_id` | `_airbyte_emitted_at` | `user_id` | `name` |
| :--- | :--- | :--- | :--- |
| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | 123 | `{ "first": "John", "last": "Doe" }` |

## Getting started

### Requirements

1. Allow connections from Airbyte server to your AWS cluster \(if they exist in separate VPCs\).
2. An S3 bucket with credentials \(for the COPY strategy\).

### Setup guide

* Fill up S3 info
  * **S3 Bucket Name**
    * See [this](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html) to create an S3 bucket.
  * **S3 Bucket Region**
  * **Access Key Id**
    * See [this](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) on how to generate an access key.
    * We recommend creating an Airbyte-specific user. This user will require [read and write permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_rw-bucket.html) to objects in the staging bucket.
  * **Secret Access Key**
    * Corresponding key to the above key id.
* Make sure your S3 bucket is accessible from the machine running Airbyte.
  * This depends on your networking setup.
  * You can check AWS S3 documentation with a tutorial on how to properly configure your S3's access [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html).
  * The easiest way to verify if Airbyte is able to connect to your S3 bucket is via the check connection tool in the UI.
