# S3

This page contains the setup guide and reference information for the Amazon S3 source connector.

:::info
Cloud storage may incur egress costs. Egress refers to data that is transferred out of the cloud storage system, such as when you download files or access them from a different location. For more information, see the [Amazon S3 pricing guide](https://aws.amazon.com/s3/pricing/).
:::

## Prerequisites

Define file pattern, see the [Path Patterns section](s3.md#path-patterns)

## Setup guide

### Step 1: Set up Amazon S3

* If syncing from a private bucket, the credentials you use for the connection must have have both `read` and `list` access on the S3 bucket. `list` is required to discover files based on the provided pattern\(s\).

### Step 2: Set up the Amazon S3 connector in Airbyte

<!-- env:cloud -->
**For Airbyte Cloud:**

1. [Log into your Airbyte Cloud](https://cloud.airbyte.com/workspaces) account.
2. In the left navigation bar, click **<Sources/Destinations>**. In the top-right corner, click **+new source/destination**.
3. On the Set up the source/destination page, enter the name for the `connector name` connector and select **connector name** from the `Source/Destination` type dropdown.
4. Set `dataset` appropriately. This will be the name of the table in the destination.
5. If your bucket contains _only_ files containing data for this table, use `**` as path\_pattern. See the [Path Patterns section](s3.md#path-patterns) for more specific pattern matching.
6. Leave schema as `{}` to automatically infer it from the file\(s\). For details on providing a schema, see the [User Schema section](s3.md#user-schema).
7. Fill in the fields within the provider box appropriately. If your bucket is not public, add [credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) with sufficient permissions under `aws_access_key_id` and `aws_secret_access_key`.
8. Choose the format corresponding to the format of your files and fill in fields as required. If unsure about values, try out the defaults and come back if needed. Find details on these settings [here](s3.md#file-format-settings).
<!-- /env:cloud -->

<!-- env:oss -->
**For Airbyte Open Source:**

1. Create a new S3 source with a suitable name. Since each S3 source maps to just a single table, it may be worth including that in the name.
2. Set `dataset` appropriately. This will be the name of the table in the destination.
3. If your bucket contains _only_ files containing data for this table, use `**` as path\_pattern. See the [Path Patterns section](s3.md#path-patterns) for more specific pattern matching.
4. Leave schema as `{}` to automatically infer it from the file\(s\). For details on providing a schema, see the [User Schema section](s3.md#user-schema).
5. Fill in the fields within the provider box appropriately. If your bucket is not public, add [credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) with sufficient permissions under `aws_access_key_id` and `aws_secret_access_key`.
6. Choose the format corresponding to the format of your files and fill in fields as required. If unsure about values, try out the defaults and come back if needed. Find details on these settings [here](s3.md#file-format-settings).
<!-- /env:oss -->


## Supported sync modes

The Amazon S3 source connector supports the following [sync modes](https://docs.airbyte.com/cloud/core-concepts#connection-sync-modes):

| Feature                                        | Supported? |
| :--------------------------------------------- | :--------- |
| Full Refresh Sync                              | Yes        |
| Incremental Sync                               | Yes        |
| Replicate Incremental Deletes                  | No         |
| Replicate Multiple Files \(pattern matching\)  | Yes        |
| Replicate Multiple Streams \(distinct tables\) | No         |
| Namespaces                                     | No         |


## File Compressions

| Compression | Supported? |
| :---------- | :--------- |
| Gzip        | Yes        |
| Zip         | No         |
| Bzip2       | Yes        |
| Lzma        | No         |
| Xz          | No         |
| Snappy      | No         |

Please let us know any specific compressions you'd like to see support for next!


## Path Patterns

\(tl;dr -&gt; path pattern syntax using [wcmatch.glob](https://facelessuser.github.io/wcmatch/glob/). GLOBSTAR and SPLIT flags are enabled.\)

This connector can sync multiple files by using glob-style patterns, rather than requiring a specific path for every file. This enables:

* Referencing many files with just one pattern, e.g. `**` would indicate every file in the bucket.
* Referencing future files that don't exist yet \(and therefore don't have a specific path\).

You must provide a path pattern. You can also provide many patterns split with \| for more complex directory layouts.

Each path pattern is a reference from the _root_ of the bucket, so don't include the bucket name in the pattern\(s\).

Some example patterns:

* `**` : match everything.
* `**/*.csv` : match all files with specific extension.
* `myFolder/**/*.csv` : match all csv files anywhere under myFolder.
* `*/**` : match everything at least one folder deep.
* `*/*/*/**` : match everything at least three folders deep.
* `**/file.*|**/file` : match every file called "file" with any extension \(or no extension\).
* `x/*/y/*` : match all files that sit in folder x -&gt; any folder -&gt; folder y.
* `**/prefix*.csv` : match all csv files with specific prefix.
* `**/prefix*.parquet` : match all parquet files with specific prefix.

Let's look at a specific example, matching the following bucket layout:

```text
myBucket
    -> log_files
    -> some_table_files
        -> part1.csv
        -> part2.csv
    -> images
    -> more_table_files
        -> part3.csv
    -> extras
        -> misc
            -> another_part1.csv
```

We want to pick up part1.csv, part2.csv and part3.csv \(excluding another\_part1.csv for now\). We could do this a few different ways:

* We could pick up every csv file called "partX" with the single pattern `**/part*.csv`.
* To be a bit more robust, we could use the dual pattern `some_table_files/*.csv|more_table_files/*.csv` to pick up relevant files only from those exact folders.
* We could achieve the above in a single pattern by using the pattern `*table_files/*.csv`. This could however cause problems in the future if new unexpected folders started being created.
* We can also recursively wildcard, so adding the pattern `extras/**/*.csv` would pick up any csv files nested in folders below "extras", such as "extras/misc/another\_part1.csv".

As you can probably tell, there are many ways to achieve the same goal with path patterns. We recommend using a pattern that ensures clarity and is robust against future additions to the directory structure.


## User Schema

Providing a schema allows for more control over the output of this stream. Without a provided schema, columns and datatypes will be inferred from the first created file in the bucket matching your path pattern and suffix. This will probably be fine in most cases but there may be situations you want to enforce a schema instead, e.g.:

* You only care about a specific known subset of the columns. The other columns would all still be included, but packed into the `_ab_additional_properties` map.
* Your initial dataset is quite small \(in terms of number of records\), and you think the automatic type inference from this sample might not be representative of the data in the future.
* You want to purposely define types for every column.
* You know the names of columns that will be added to future data and want to include these in the core schema as columns rather than have them appear in the `_ab_additional_properties` map.

Or any other reason! The schema must be provided as valid JSON as a map of `{"column": "datatype"}` where each datatype is one of:

* string
* number
* integer
* object
* array
* boolean
* null

For example:

* {"id": "integer", "location": "string", "longitude": "number", "latitude": "number"}
* {"username": "string", "friends": "array", "information": "object"}

:::note

Please note, the S3 Source connector used to infer schemas from all the available files and then merge them to create a superset schema. Starting from version 2.0.0 the schema inference works based on the first file found only. The first file we consider is the oldest one written to the prefix.

:::


## S3 Provider Settings

* `bucket` : name of the bucket your files are in
* `aws_access_key_id` : one half of the [required credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) for accessing a private bucket.
* `aws_secret_access_key` : other half of the [required credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) for accessing a private bucket.
* `path_prefix` : an optional string that limits the files returned by AWS when listing files to only that those starting with this prefix. This is different to path\_pattern as it gets pushed down to the API call made to S3 rather than filtered in Airbyte and it does not accept pattern-style symbols \(like wildcards `*`\). We recommend using this if your bucket has many folders and files that are unrelated to this stream and all the relevant files will always sit under this chosen prefix.
  * Together with `path_pattern`, there are multiple ways to specify the files to sync. For example, all the following configs are equivalent:
    * `path_prefix` = `<empty>`, `path_pattern` = `path1/path2/myFolder/**/*`.
    * `path_prefix` = `path1/`, `path_pattern` = `path2/myFolder/**/*.csv`.
    * `path_prefix` = `path1/path2/` and `path_pattern` = `myFolder/**/*.csv`
    * `path_prefix` = `path1/path2/myFolder/`, `path_pattern` = `**/*.csv`. This is the most efficient one because the directories are filtered earlier in the S3 API call. However, the difference in efficiency is usually negligible.
  * The rationale of having both `path_prefix` and `path_pattern` is to accommodate as many use cases as possible. If you found them confusing, feel free to ignore `path_prefix` and just set the `path_pattern`.
* `endpoint` : optional parameter that allow using of non Amazon S3 compatible services. Leave it blank for using default Amazon serivce.
* `use_ssl` : Allows using custom servers that configured to use plain http. Ignored in case of using Amazon service.
* `verify_ssl_cert` : Skip ssl validity check in case of using custom servers with self signed certificates. Ignored in case of using Amazon service.

  **File Format Settings**

  The Reader in charge of loading the file format is currently based on [PyArrow](https://arrow.apache.org/docs/python/generated/pyarrow.csv.open_csv.html) \(Apache Arrow\).

  Note that all files within one stream must adhere to the same read options for every provided format.

### CSV

Since CSV files are effectively plain text, providing specific reader options is often required for correct parsing of the files. These settings are applied when a CSV is created or exported so please ensure that this process happens consistently over time.

* `delimiter` : Even though CSV is an acronymn for Comma Separated Values, it is used more generally as a term for flat file data that may or may not be comma separated. The delimiter field lets you specify which character acts as the separator.
* `quote_char` : In some cases, data values may contain instances of reserved characters \(like a comma, if that's the delimiter\). CSVs can allow this behaviour by wrapping a value in defined quote characters so that on read it can parse it correctly.
* `escape_char` : An escape character can be used to prefix a reserved character and allow correct parsing.
* `encoding` : Some data may use a different character set \(typically when different alphabets are involved\). See the [list of allowable encodings here](https://docs.python.org/3/library/codecs.html#standard-encodings).
* `double_quote` : Whether two quotes in a quoted CSV value denote a single quote in the data.
* `newlines_in_values` : Sometimes referred to as `multiline`. In most cases, newline characters signal the end of a row in a CSV, however text data may contain newline characters within it. Setting this to True allows correct parsing in this case.
* `block_size` : This is the number of bytes to process in memory at a time while reading files. The default value here is usually fine but if your table is particularly wide \(lots of columns / data in fields is large\) then raising this might solve failures on detecting schema. Since this defines how much data to read into memory, raising this too high could cause Out Of Memory issues so use with caution.
* `additional_reader_options` : This allows for editing the less commonly required CSV [ConvertOptions](https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions). The value must be a valid JSON string, e.g.:

    ```text
    {"timestamp_parsers": ["%m/%d/%Y %H:%M", "%Y/%m/%d %H:%M"], "strings_can_be_null": true, "null_values": ["NA", "NULL"]}
    ```
* `advanced_options` : This allows for editing the less commonly required CSV [ReadOptions](https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions). The value must be a valid JSON string. One use case for this is when your CSV has no header, or you want to use custom column names, you can specify `column_names` using this option.

    ```test
    {"column_names": ["column1", "column2", "column3"]}
    ```

### Parquet

Apache Parquet file is a column-oriented data storage format of the Apache Hadoop ecosystem. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. For now, the solution involves iterating through individual files at the abstract level thus partitioned parquet datasets are unsupported. The following settings are available:

* `buffer_size` : If positive, perform read buffering when deserializing individual column chunks. Otherwise IO calls are unbuffered.
* `columns` : If not None, only these columns will be read from the file.
* `batch_size` : Maximum number of records per batch. Batches may be smaller if there aren’t enough rows in the file.

You can find details on [here](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html#pyarrow.parquet.ParquetFile.iter_batches).

### Avro

The avro parser uses [fastavro](https://fastavro.readthedocs.io/en/latest/). Currently, no additional options are supported.

### Jsonl

The Jsonl parser uses pyarrow hence,only the line-delimited JSON format is supported.For more detailed info, please refer to the [docs](https://arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html)

## Changelog

| Version | Date       | Pull Request                                                                                                    | Subject                                                                                                              |
|:--------|:-----------|:----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|
| 3.0.0   | 2023-05-02 | [25127](https://github.com/airbytehq/airbyte/pull/25127)                                                        | Remove ab_additional column; Use platform-handled schema evolution                                                   |
| 2.2.0   | 2023-05-10 | [25937](https://github.com/airbytehq/airbyte/pull/25937)                                                        | Add support for Parquet Dataset                                                                                      |
| 2.1.4   | 2023-05-01 | [25361](https://github.com/airbytehq/airbyte/pull/25361)                                                        | Parse nested avro schemas                                                                                            |
| 2.1.3   | 2023-05-01 | [25706](https://github.com/airbytehq/airbyte/pull/25706)                                                        | Remove minimum block size for CSV check                                                                              |
| 2.1.2   | 2023-04-18 | [25067](https://github.com/airbytehq/airbyte/pull/25067)                                                        | Handle block size related errors; fix config validator                                                               |
| 2.1.1   | 2023-04-18 | [25010](https://github.com/airbytehq/airbyte/pull/25010)                                                        | Refactor filter logic                                                                                                |
| 2.1.0   | 2023-04-10 | [25010](https://github.com/airbytehq/airbyte/pull/25010)                                                        | Add `start_date` field to filter files based on `LastModified` option                                                |
| 2.0.4   | 2023-03-23 | [24429](https://github.com/airbytehq/airbyte/pull/24429)                                                        | Call `check` with a little block size to save time and memory.                                                       |
| 2.0.3   | 2023-03-17 | [24178](https://github.com/airbytehq/airbyte/pull/24178)                                                        | Support legacy datetime format for the period of migration, fix time-zone conversion.                                |
| 2.0.2   | 2023-03-16 | [24157](https://github.com/airbytehq/airbyte/pull/24157)                                                        | Return empty schema if `discover` finds no files; Do not infer extra data types when user defined schema is applied. |
| 2.0.1   | 2023-03-06 | [23195](https://github.com/airbytehq/airbyte/pull/23195)                                                        | Fix datetime format string                                                                                           |
| 2.0.0   | 2023-03-14 | [23189](https://github.com/airbytehq/airbyte/pull/23189)                                                        | Infer schema based on one file instead of all the files                                                              |
| 1.0.2   | 2023-03-02 | [23669](https://github.com/airbytehq/airbyte/pull/23669)                                                        | Made `Advanced Reader Options` and `Advanced Options` truly `optional` for `CSV` format                              |
| 1.0.1   | 2023-02-27 | [23502](https://github.com/airbytehq/airbyte/pull/23502)                                                        | Fix error handling                                                                                                   |
| 1.0.0   | 2023-02-17 | [23198](https://github.com/airbytehq/airbyte/pull/23198)                                                        | Fix Avro schema discovery                                                                                            |
| 0.1.32  | 2023-02-07 | [22500](https://github.com/airbytehq/airbyte/pull/22500)                                                        | Speed up discovery                                                                                                   |
| 0.1.31  | 2023-02-08 | [22550](https://github.com/airbytehq/airbyte/pull/22550)                                                        | Validate CSV read options and convert options                                                                        |
| 0.1.30  | 2023-01-25 | [21587](https://github.com/airbytehq/airbyte/pull/21587)                                                        | Make sure spec works as expected in UI                                                                               |
| 0.1.29  | 2023-01-19 | [21604](https://github.com/airbytehq/airbyte/pull/21604)                                                        | Handle OSError: skip unreachable keys and keep working on accessible ones. Warn a customer                           |
| 0.1.28  | 2023-01-10 | [21210](https://github.com/airbytehq/airbyte/pull/21210)                                                        | Update block size for json file format                                                                               |
| 0.1.27  | 2022-12-08 | [20262](https://github.com/airbytehq/airbyte/pull/20262)                                                        | Check config settings for CSV file format                                                                            |
| 0.1.26  | 2022-11-08 | [19006](https://github.com/airbytehq/airbyte/pull/19006)                                                        | Add virtual-hosted-style option                                                                                      |
| 0.1.24  | 2022-10-28 | [18602](https://github.com/airbytehq/airbyte/pull/18602)                                                        | Wrap errors into AirbyteTracedException pointing to a problem file                                                   |
| 0.1.23  | 2022-10-10 | [17991](https://github.com/airbytehq/airbyte/pull/17991)                                                        | Fix pyarrow to JSON schema type conversion for arrays                                                                |
| 0.1.23  | 2022-10-10 | [17800](https://github.com/airbytehq/airbyte/pull/17800)                                                        | Deleted `use_ssl` and `verify_ssl_cert` flags and hardcoded to `True`                                                |
| 0.1.22  | 2022-09-28 | [17304](https://github.com/airbytehq/airbyte/pull/17304)                                                        | Migrate to per-stream state                                                                                          |
| 0.1.21  | 2022-09-20 | [16921](https://github.com/airbytehq/airbyte/pull/16921)                                                        | Upgrade pyarrow                                                                                                      |
| 0.1.20  | 2022-09-12 | [16607](https://github.com/airbytehq/airbyte/pull/16607)                                                        | Fix for reading jsonl files containing nested structures                                                             |
| 0.1.19  | 2022-09-13 | [16631](https://github.com/airbytehq/airbyte/pull/16631)                                                        | Adjust column type to a broadest one when merging two or more json schemas                                           |
| 0.1.18  | 2022-08-01 | [14213](https://github.com/airbytehq/airbyte/pull/14213)                                                        | Add support for jsonl format files.                                                                                  |
| 0.1.17  | 2022-07-21 | [14911](https://github.com/airbytehq/airbyte/pull/14911)                                                        | "decimal" type added for parquet                                                                                     |
| 0.1.16  | 2022-07-13 | [14669](https://github.com/airbytehq/airbyte/pull/14669)                                                        | Fixed bug when extra columns apeared to be non-present in master schema                                              |
| 0.1.15  | 2022-05-31 | [12568](https://github.com/airbytehq/airbyte/pull/12568)                                                        | Fixed possible case of files being missed during incremental syncs                                                   |
| 0.1.14  | 2022-05-23 | [11967](https://github.com/airbytehq/airbyte/pull/11967)                                                        | Increase unit test coverage up to 90%                                                                                |
| 0.1.13  | 2022-05-11 | [12730](https://github.com/airbytehq/airbyte/pull/12730)                                                        | Fixed empty options issue                                                                                            |
| 0.1.12  | 2022-05-11 | [12602](https://github.com/airbytehq/airbyte/pull/12602)                                                        | Added support for Avro file format                                                                                   |
| 0.1.11  | 2022-04-30 | [12500](https://github.com/airbytehq/airbyte/pull/12500)                                                        | Improve input configuration copy                                                                                     |
| 0.1.10  | 2022-01-28 | [8252](https://github.com/airbytehq/airbyte/pull/8252)                                                          | Refactoring of files' metadata                                                                                       |
| 0.1.9   | 2022-01-06 | [9163](https://github.com/airbytehq/airbyte/pull/9163)                                                          | Work-around for web-UI, `backslash - t` converts to `tab` for `format.delimiter` field.                              |
| 0.1.7   | 2021-11-08 | [7499](https://github.com/airbytehq/airbyte/pull/7499)                                                          | Remove base-python dependencies                                                                                      |
| 0.1.6   | 2021-10-15 | [6615](https://github.com/airbytehq/airbyte/pull/6615) & [7058](https://github.com/airbytehq/airbyte/pull/7058) | Memory and performance optimisation. Advanced options for CSV parsing.                                               |
| 0.1.5   | 2021-09-24 | [6398](https://github.com/airbytehq/airbyte/pull/6398)                                                          | Support custom non Amazon S3 services                                                                                |
| 0.1.4   | 2021-08-13 | [5305](https://github.com/airbytehq/airbyte/pull/5305)                                                          | Support of Parquet format                                                                                            |
| 0.1.3   | 2021-08-04 | [5197](https://github.com/airbytehq/airbyte/pull/5197)                                                          | Fixed bug where sync could hang indefinitely on schema inference                                                     |
| 0.1.2   | 2021-08-02 | [5135](https://github.com/airbytehq/airbyte/pull/5135)                                                          | Fixed bug in spec so it displays in UI correctly                                                                     |
| 0.1.1   | 2021-07-30 | [4990](https://github.com/airbytehq/airbyte/pull/4990/commits/ff5f70662c5f84eabc03526cddfcc9d73c58c0f4)         | Fixed documentation url in source definition                                                                         |
| 0.1.0   | 2021-07-30 | [4990](https://github.com/airbytehq/airbyte/pull/4990)                                                          | Created S3 source connector                                                                                          |
