{
  "documentationUrl": "https://docs.airbyte.io/integrations/destinations/redshift",
  "supportsIncremental": true,
  "supportsNormalization": true,
  "supportsDBT": true,
  "supported_destination_sync_modes": ["overwrite", "append", "append_dedup"],
  "connectionSpecification": {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Redshift Destination Spec",
    "type": "object",
    "required": ["host", "port", "database", "username", "password", "schema"],
    "additionalProperties": true,
    "properties": {
      "host": {
        "description": "Host Endpoint of the Redshift Cluster (must include the cluster-id, region and end with .redshift.amazonaws.com)",
        "type": "string",
        "title": "Host"
      },
      "port": {
        "description": "Port of the database.",
        "type": "integer",
        "minimum": 0,
        "maximum": 65536,
        "default": 5439,
        "examples": ["5439"],
        "title": "Port"
      },
      "username": {
        "description": "Username to use to access the database.",
        "type": "string",
        "title": "Username"
      },
      "password": {
        "description": "Password associated with the username.",
        "type": "string",
        "airbyte_secret": true,
        "title": "Password"
      },
      "database": {
        "description": "Name of the database.",
        "type": "string",
        "title": "Database"
      },
      "schema": {
        "description": "The default schema tables are written to if the source does not specify a namespace. Unless specifically configured, the usual value for this field is \"public\".",
        "type": "string",
        "examples": ["public"],
        "default": "public",
        "title": "Default Schema"
      },
      "s3_bucket_name": {
        "title": "S3 Bucket Name",
        "type": "string",
        "description": "The name of the staging S3 bucket to use if utilising a COPY strategy. COPY is recommended for production workloads for better speed and scalability. See <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html\">AWS docs</a> for more details.",
        "examples": ["airbyte.staging"]
      },
      "s3_bucket_path": {
        "title": "S3 Bucket Path",
        "type": "string",
        "description": "The directory under the S3 bucket where data will be written. If not provided, then defaults to the root directory.",
        "examples": ["data_sync/test"]
      },
      "s3_bucket_region": {
        "title": "S3 Bucket Region",
        "type": "string",
        "default": "",
        "description": "The region of the S3 staging bucket to use if utilising a copy strategy.",
        "enum": [
          "",
          "us-east-1",
          "us-east-2",
          "us-west-1",
          "us-west-2",
          "af-south-1",
          "ap-east-1",
          "ap-south-1",
          "ap-northeast-1",
          "ap-northeast-2",
          "ap-northeast-3",
          "ap-southeast-1",
          "ap-southeast-2",
          "ca-central-1",
          "cn-north-1",
          "cn-northwest-1",
          "eu-central-1",
          "eu-north-1",
          "eu-south-1",
          "eu-west-1",
          "eu-west-2",
          "eu-west-3",
          "sa-east-1",
          "me-south-1"
        ]
      },
      "access_key_id": {
        "type": "string",
        "description": "The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.",
        "title": "S3 Key Id",
        "airbyte_secret": true
      },
      "secret_access_key": {
        "type": "string",
        "description": "The corresponding secret to the above access key id.",
        "title": "S3 Access Key",
        "airbyte_secret": true
      },
      "part_size": {
        "type": "integer",
        "minimum": 10,
        "maximum": 100,
        "examples": ["10"],
        "description": "Optional. Increase this if syncing tables larger than 100GB. Only relevant for COPY. Files are streamed to S3 in parts. This determines the size of each part, in MBs. As S3 has a limit of 10,000 parts per file, part size affects the table size. This is 10MB by default, resulting in a default limit of 100GB tables. Note, a larger part size will result in larger memory requirements. A rule of thumb is to multiply the part size by 10 to get the memory requirement. Modify this with care.",
        "title": "Stream Part Size"
      },
      "purge_staging_data": {
        "title": "Purge Staging Files and Tables",
        "type": "boolean",
        "description": "Whether to delete the staging files from S3 after completing the sync. See the docs for details. Only relevant for COPY. Defaults to true.",
        "default": true
      }
    }
  }
}
