# Copyright (c) 2024 Airbyte, Inc., all rights reserved.

import argparse
import csv
import json
import os

import ibm_db_dbi


# ===============================================
# command to run this script:
# python cdc_setup_db2.py --database <DATABASE> --host <HOST> --port <PORT> --user <USER> --password <PASSWORD> --schema <SOURCE_SCHEMA>
# ===============================================


def get_connection(database, host, port, user, password):
    """Establishes a connection to DB2."""
    conn_str = f"DATABASE={database};HOSTNAME={host};PORT={port};PROTOCOL=TCPIP;UID={user};PWD={password};"
    print(conn_str)
    return ibm_db_dbi.connect(conn_str, "", "")


def check_cdc_table_exists(conn, cdc_schema, cdc_table):
    """Checks if the specific CDC table exists."""
    cursor = conn.cursor()
    query = "SELECT COUNT(*) FROM SYSCAT.TABLES WHERE TABSCHEMA = ? AND TABNAME = ?"
    count = 0
    try:
        cursor.execute(query, (cdc_schema, cdc_table))
        count = cursor.fetchone()[0]
    except Exception as e:
        print(f"Warning: Error checking if table exists {cdc_schema}.{cdc_table}: {e}")
    finally:
        cursor.close()
    return count > 0


def create_cdc_table(conn, cdc_schema, cdc_table, columns_with_types):
    """Creates the CDC table with before/after columns for each source column."""
    cursor = conn.cursor()
    columns = [
        '"_ab_trigger_change_id" BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY',
        '"_ab_trigger_change_time" TIMESTAMP',
        '"_ab_trigger_operation_type" NVARCHAR(10)',
    ]

    for col in columns_with_types:
        col_name = col["name"]
        data_type = col["type"]
        safe_col_name = col_name.replace('"', '""')
        columns.append(f'"_ab_trigger_{safe_col_name}_before" {data_type}')
        columns.append(f'"_ab_trigger_{safe_col_name}_after" {data_type}')

    ddl = f'CREATE TABLE "{cdc_schema}"."{cdc_table}" (\n    {", ".join(columns)}\n)'

    try:
        cursor.execute(ddl)
        conn.commit()
        print(f"Successfully created CDC table {cdc_schema}.{cdc_table}")
    except Exception as e:
        print(f"Unexpected error creating CDC table {cdc_schema}.{cdc_table}: {e}")
        conn.rollback()
    finally:
        cursor.close()


def get_table_columns_with_types(conn, schema, table):
    """Gets column names and their full data types."""
    cursor = conn.cursor()
    query = """
        SELECT COLNAME, TYPENAME, LENGTH, SCALE
        FROM SYSCAT.COLUMNS
        WHERE TABSCHEMA = ? AND TABNAME = ?
        ORDER BY COLNO
    """
    columns = []
    try:
        cursor.execute(query, (schema, table))
        for row in cursor.fetchall():
            col_name, data_type, length, scale = row
            full_type = data_type

            # Handle type-specific attributes
            if data_type in ["VARCHAR", "CHARACTER", "BINARY", "VARBINARY"]:
                if length is not None:
                    full_type += f"({length})"
            elif data_type in ["DECIMAL", "NUMERIC"]:
                if scale is not None:
                    full_type += f"({length},{scale})"  # Use LENGTH for precision
                elif length is not None:
                    full_type += f"({length})"

            columns.append({"name": col_name, "type": full_type})
    except Exception as e:
        print(f"Error getting columns for {schema}.{table}: {e}")
    finally:
        cursor.close()
    return columns


def check_trigger_exists(conn, schema_name, trigger_name):
    """Checks if a trigger exists."""
    cursor = conn.cursor()
    query = "SELECT COUNT(*) FROM SYSCAT.TRIGGERS WHERE TRIGSCHEMA = ? AND TRIGNAME = ?"
    count = 0
    try:
        cursor.execute(query, (schema_name, trigger_name))
        count = cursor.fetchone()[0]
    except Exception as e:
        print(f"Warning: Error checking trigger {schema_name}.{trigger_name}: {e}")
    finally:
        cursor.close()
    return count > 0


def create_single_trigger(conn, recreate_trigger, operation_type, source_schema, source_table, cdc_schema, cdc_table, columns_with_types):
    """Creates a single trigger for the specified operation."""
    trigger_name = f"TRG_{source_schema}_{source_table}_CDC_{operation_type[:3].upper()}"
    if check_trigger_exists(conn, source_schema, trigger_name):
        if recreate_trigger:
            drop_trigger(conn, source_schema, trigger_name)
            print(f'Dropped trigger "{source_schema}"."{trigger_name}"')
        else:
            print(f"Trigger {trigger_name} exists. Skipping.")
            return

    columns = ['"_ab_trigger_change_time"', '"_ab_trigger_operation_type"']
    values = ["CURRENT TIMESTAMP", f"'{operation_type}'"]

    if operation_type == "INSERT":
        referencing = "REFERENCING NEW AS N"
    elif operation_type == "UPDATE":
        referencing = "REFERENCING OLD AS O NEW AS N"
    elif operation_type == "DELETE":
        referencing = "REFERENCING OLD AS O"
    else:
        print(f"Invalid operation type: {operation_type}")
        return

    for col in columns_with_types:
        col_name = col["name"]
        safe_col = col_name.replace('"', '""')
        if operation_type in ["INSERT", "UPDATE"]:
            columns.append(f'"_ab_trigger_{safe_col}_after"')
            values.append(f'N."{safe_col}"' if operation_type != "UPDATE" else f'N."{safe_col}"')
        if operation_type in ["UPDATE", "DELETE"]:
            columns.append(f'"_ab_trigger_{safe_col}_before"')
            values.append(f'O."{safe_col}"')

    columns_str = ", ".join(columns)
    values_str = ", ".join(values)

    ddl = f"""
        CREATE TRIGGER "{source_schema}"."{trigger_name}"
        AFTER {operation_type} ON "{source_schema}"."{source_table}"
        {referencing}
        FOR EACH ROW
        INSERT INTO "{cdc_schema}"."{cdc_table}" (
            {columns_str}
        )
        VALUES (
            {values_str}
        )
    """

    cursor = conn.cursor()
    try:
        cursor.execute(ddl)
        conn.commit()
        print(f"Created trigger {trigger_name}")
    except Exception as e:
        print(f"Error creating trigger {trigger_name}: {e}")
        conn.rollback()
    finally:
        cursor.close()


def drop_trigger(conn, schema_name, trigger_name):
    """Drops a trigger."""
    cursor = conn.cursor()
    query = f'DROP TRIGGER "{schema_name}"."{trigger_name}"'
    try:
        cursor.execute(query)
    except Exception as e:
        print(f"Error dropping trigger {schema_name}.{trigger_name}: {e}")
    finally:
        cursor.close()


def get_tables_from_schema(conn, schema):
    """Retrieves tables from a schema."""
    cursor = conn.cursor()
    query = "SELECT TABNAME FROM SYSCAT.TABLES WHERE TABSCHEMA = ? AND TYPE IN ('H', 'T', 'U')"
    tables = []
    try:
        cursor.execute(query, (schema,))
        tables = [{"schema": schema, "table": row[0]} for row in cursor.fetchall()]
    except Exception as e:
        print(f"Error fetching tables for schema {schema}: {e}")
    finally:
        cursor.close()
    return tables


def get_tables_from_file(input_file):
    """Reads tables from CSV/JSON file."""
    tables = []
    ext = os.path.splitext(input_file)[1].lower()
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            if ext == ".csv":
                reader = csv.DictReader(f)
                for row in reader:
                    tables.append({"schema": row["schema"], "table": row["table"]})
            elif ext == ".json":
                data = json.load(f)
                tables = [{"schema": item["schema"], "table": item["table"]} for item in data]
    except Exception as e:
        print(f"Error reading input file: {e}")
        exit(1)
    return tables


def main():
    parser = argparse.ArgumentParser(description="Create CDC triggers in Db2")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--schema", help="Process all tables in a schema")
    group.add_argument("--input-file", help="CSV/JSON file with tables")
    parser.add_argument("--tables", nargs="+", help="List of table names to process (requires --schema)")
    parser.add_argument("--database", required=True)
    parser.add_argument("--host", required=True)
    parser.add_argument("--port", required=True)
    parser.add_argument("--user", required=True)
    parser.add_argument("--password", required=True)
    parser.add_argument("--cdc-schema", default="_ab_cdc")
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument("--recreate-triggers", default=False)
    args = parser.parse_args()
    print(args)

    try:
        conn = get_connection(args.database, args.host, args.port, args.user, args.password)
    except Exception as e:
        print(f"Connection failed: {e}")
        exit(1)

    tables = []
    if args.schema and args.tables:
        # Process specific tables in the given schema
        tables = [{"schema": args.schema, "table": table} for table in args.tables]
    elif args.schema:
        # Process all tables in the schema
        tables = get_tables_from_schema(conn, args.schema)
    elif args.input_file:
        # Process tables from the input file
        tables = get_tables_from_file(args.input_file)

    for table in tables:
        source_schema = table["schema"]
        source_table = table["table"]
        cdc_table = f"_ab_trigger_{source_schema}_{source_table}"

        columns = get_table_columns_with_types(conn, source_schema, source_table)
        if not columns:
            continue

        if not check_cdc_table_exists(conn, args.cdc_schema, cdc_table):
            create_cdc_table(conn, args.cdc_schema, cdc_table, columns)

        for op in ["INSERT", "UPDATE", "DELETE"]:
            create_single_trigger(conn, args.recreate_triggers, op, source_schema, source_table, args.cdc_schema, cdc_table, columns)

    print("done")
    conn.close()


if __name__ == "__main__":
    main()
