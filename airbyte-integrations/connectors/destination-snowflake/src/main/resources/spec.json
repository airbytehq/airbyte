{
  "documentationUrl": "https://docs.airbyte.io/integrations/destinations/snowflake",
  "supportsIncremental": true,
  "supportsNormalization": true,
  "supportsDBT": true,
  "supported_destination_sync_modes": ["overwrite", "append", "append_dedup"],
  "connectionSpecification": {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Snowflake Destination Spec",
    "type": "object",
    "required": [
      "host",
      "role",
      "warehouse",
      "database",
      "schema",
      "username",
      "password"
    ],
    "additionalProperties": true,
    "properties": {
      "host": {
        "description": "Host domain of the snowflake instance (must include the account, region, cloud environment, and end with snowflakecomputing.com).",
        "examples": ["accountname.us-east-2.aws.snowflakecomputing.com"],
        "type": "string",
        "title": "Host",
        "order": 0
      },
      "role": {
        "description": "The role you created for Airbyte to access Snowflake.",
        "examples": ["AIRBYTE_ROLE"],
        "type": "string",
        "title": "Role",
        "order": 1
      },
      "warehouse": {
        "description": "The warehouse you created for Airbyte to sync data into.",
        "examples": ["AIRBYTE_WAREHOUSE"],
        "type": "string",
        "title": "Warehouse",
        "order": 2
      },
      "database": {
        "description": "The database you created for Airbyte to sync data into.",
        "examples": ["AIRBYTE_DATABASE"],
        "type": "string",
        "title": "Database",
        "order": 3
      },
      "schema": {
        "description": "The default Snowflake schema tables are written to if the source does not specify a namespace. Schema name would be transformed to allowed by Snowflake if it not follow Snowflake Naming Conventions https://docs.airbyte.io/integrations/destinations/snowflake#notes-about-snowflake-naming-conventions ",
        "examples": ["AIRBYTE_SCHEMA"],
        "type": "string",
        "title": "Default Schema",
        "order": 4
      },
      "username": {
        "description": "The username you created to allow Airbyte to access the database.",
        "examples": ["AIRBYTE_USER"],
        "type": "string",
        "title": "Username",
        "order": 5
      },
      "password": {
        "description": "Password associated with the username.",
        "type": "string",
        "airbyte_secret": true,
        "title": "Password",
        "order": 6
      },
      "loading_method": {
        "type": "object",
        "title": "Loading Method",
        "description": "Loading method used to send data to Snowflake.",
        "order": 7,
        "oneOf": [
          {
            "title": "[Recommended] Internal Staging",
            "additionalProperties": false,
            "description": "Writes large batches of records to a file, uploads the file to Snowflake, then uses <pre>COPY INTO table</pre> to upload the file. Recommended for large production workloads for better speed and scalability.",
            "required": ["method"],
            "properties": {
              "method": {
                "type": "string",
                "enum": ["Internal Staging"],
                "default": "Internal Staging"
              }
            }
          },
          {
            "title": "Standard Inserts",
            "additionalProperties": false,
            "description": "Uses <pre>INSERT</pre> statements to send batches of records to Snowflake. Easiest (no setup) but not recommended for large production workloads due to slow speed.",
            "required": ["method"],
            "properties": {
              "method": {
                "type": "string",
                "enum": ["Standard"],
                "default": "Standard"
              }
            }
          },
          {
            "title": "AWS S3 Staging",
            "additionalProperties": false,
            "description": "Writes large batches of records to a file, uploads the file to S3, then uses <pre>COPY INTO table</pre> to upload the file. Recommended for large production workloads for better speed and scalability.",
            "required": [
              "method",
              "s3_bucket_name",
              "access_key_id",
              "secret_access_key"
            ],
            "properties": {
              "method": {
                "type": "string",
                "enum": ["S3 Staging"],
                "default": "S3 Staging",
                "order": 0
              },
              "s3_bucket_name": {
                "title": "S3 Bucket Name",
                "type": "string",
                "description": "The name of the staging S3 bucket. Airbyte will write files to this bucket and read them via <pre>COPY</pre> statements on Snowflake.",
                "examples": ["airbyte.staging"],
                "order": 1
              },
              "s3_bucket_region": {
                "title": "S3 Bucket Region",
                "type": "string",
                "default": "",
                "description": "The region of the S3 staging bucket to use if utilising a copy strategy.",
                "enum": [
                  "",
                  "us-east-1",
                  "us-east-2",
                  "us-west-1",
                  "us-west-2",
                  "af-south-1",
                  "ap-east-1",
                  "ap-south-1",
                  "ap-northeast-1",
                  "ap-northeast-2",
                  "ap-northeast-3",
                  "ap-southeast-1",
                  "ap-southeast-2",
                  "ca-central-1",
                  "cn-north-1",
                  "cn-northwest-1",
                  "eu-central-1",
                  "eu-west-1",
                  "eu-west-2",
                  "eu-west-3",
                  "eu-south-1",
                  "eu-north-1",
                  "sa-east-1",
                  "me-south-1"
                ],
                "order": 2
              },
              "access_key_id": {
                "type": "string",
                "description": "The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.",
                "title": "S3 Key Id",
                "airbyte_secret": true,
                "order": 3
              },
              "secret_access_key": {
                "type": "string",
                "description": "The corresponding secret to the above access key id.",
                "title": "S3 Access Key",
                "airbyte_secret": true,
                "order": 4
              },
              "part_size": {
                "type": "integer",
                "default": 5,
                "examples": [5],
                "description": "Optional. Increase this if syncing tables larger than 100GB. Only relevant for COPY. Files are streamed to S3 in parts. This determines the size of each part, in MBs. As S3 has a limit of 10,000 parts per file, part size affects the table size. This is 10MB by default, resulting in a default limit of 100GB tables. Note, a larger part size will result in larger memory requirements. A rule of thumb is to multiply the part size by 10 to get the memory requirement. Modify this with care.",
                "title": "Stream Part Size",
                "order": 5
              }
            }
          },
          {
            "title": "GCS Staging",
            "additionalProperties": false,
            "description": "Writes large batches of records to a file, uploads the file to GCS, then uses <pre>COPY INTO table</pre> to upload the file. Recommended for large production workloads for better speed and scalability.",
            "required": [
              "method",
              "project_id",
              "bucket_name",
              "credentials_json"
            ],
            "properties": {
              "method": {
                "type": "string",
                "enum": ["GCS Staging"],
                "default": "GCS Staging",
                "order": 0
              },
              "project_id": {
                "title": "GCP Project ID",
                "type": "string",
                "description": "The name of the GCP project ID for your credentials.",
                "examples": ["my-project"],
                "order": 1
              },
              "bucket_name": {
                "title": "GCS Bucket Name",
                "type": "string",
                "description": "The name of the staging GCS bucket. Airbyte will write files to this bucket and read them via <pre>COPY</pre> statements on Snowflake.",
                "examples": ["airbyte-staging"],
                "order": 2
              },
              "credentials_json": {
                "title": "Google Application Credentials",
                "type": "string",
                "description": "The contents of the JSON key file that has read/write permissions to the staging GCS bucket. You will separately need to grant bucket access to your Snowflake GCP service account. See the <a href=\"https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys\">GCP docs</a> for more information on how to generate a JSON key for your service account.",
                "airbyte_secret": true,
                "multiline": true,
                "order": 3
              }
            }
          }
        ]
      }
    }
  }
}
