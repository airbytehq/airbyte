/*
 * Copyright (c) 2024 Airbyte, Inc., all rights reserved.
 */

package io.airbyte.integrations.source.sap_hana

import io.airbyte.cdk.StreamIdentifier
import io.airbyte.cdk.command.CliRunner
import io.airbyte.cdk.discover.DiscoveredStream
import io.airbyte.cdk.discover.Field
import io.airbyte.cdk.jdbc.IntFieldType
import io.airbyte.cdk.jdbc.StringFieldType
import io.airbyte.cdk.output.BufferingOutputConsumer
import io.airbyte.protocol.models.v0.AirbyteConnectionStatus
import io.airbyte.protocol.models.v0.AirbyteStateMessage
import io.airbyte.protocol.models.v0.AirbyteStream
import io.airbyte.protocol.models.v0.CatalogHelpers
import io.airbyte.protocol.models.v0.ConfiguredAirbyteCatalog
import io.airbyte.protocol.models.v0.ConfiguredAirbyteStream
import io.airbyte.protocol.models.v0.StreamDescriptor
import io.airbyte.protocol.models.v0.SyncMode
import java.sql.SQLException
import org.junit.jupiter.api.AfterEach
import org.junit.jupiter.api.Assertions.assertEquals
import org.junit.jupiter.api.BeforeEach
import org.junit.jupiter.api.Test

class SapHanaSourceCdcIntegrationTest {
    @BeforeEach
    fun setUp() {
        try {
            db.connect()
            for (i in 0..schemaNames.size - 1) {
                db.execute("CREATE SCHEMA \"${schemaNames[i]}\"")
                // Verify schema creation
                if (!db.verifySchemaExists(schemaNames[i])) {
                    throw SQLException("Failed to create schema ${schemaNames[i]}")
                }
                for (j in 0..tableNames.size - 1) {
                    // Create the main table
                    db.execute(
                        "CREATE TABLE \"${schemaNames[i]}\".\"${tableNames[j]}\" (\"ID\" INT PRIMARY KEY, \"NAME\" VARCHAR(255))"
                    )
                    // Verify table creation
                    if (!db.verifyTableExists(schemaNames[i], tableNames[j])) {
                        throw SQLException(
                            "Failed to create table \"${schemaNames[i]}\".\"${tableNames[j]}\""
                        )
                    }

                    // Create the trigger table in the _ab_cdc schema
                    try {
                        db.execute(
                            "CREATE SCHEMA \"${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}\""
                        )
                    } catch (e: SQLException) {
                        // Schema might already exist, which is fine
                    }
                    // Verify trigger schema creation
                    if (!db.verifySchemaExists(TriggerTableConfig.TRIGGER_TABLE_NAMESPACE)) {
                        throw SQLException(
                            "Failed to create schema \"${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}\""
                        )
                    }

                    val triggerTableName =
                        "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}${schemaNames[i]}_${tableNames[j]}"

                    // Create the trigger table
                    db.execute(
                        """
                        CREATE TABLE "${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}"."${triggerTableName}" (
                            "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}change_id" BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
                            "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}change_time" TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}primary_key" VARCHAR(255),
                            "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}stream_name" VARCHAR(255),
                            "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}operation_type" VARCHAR(10),
                            "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}value_before" NCLOB,
                            "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}value_after" NCLOB
                        )
                        """
                    )
                    // Verify trigger table creation
                    if (
                        !db.verifyTableExists(
                            TriggerTableConfig.TRIGGER_TABLE_NAMESPACE,
                            triggerTableName
                        )
                    ) {
                        throw SQLException(
                            "Failed to create trigger table \"${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}\".\"${triggerTableName}\""
                        )
                    }

                    // Create insert trigger
                    db.execute(
                        """
                        CREATE TRIGGER "${schemaNames[i]}"."${tableNames[j]}_before_insert"
                        BEFORE INSERT ON "${schemaNames[i]}"."${tableNames[j]}"
                        REFERENCING NEW ROW new_row
                        FOR EACH ROW
                        BEGIN
                            INSERT INTO "${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}"."${triggerTableName}" (
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}primary_key",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}stream_name",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}operation_type",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}value_after"
                            )
                            VALUES (
                                :new_row."ID",
                                '${tableNames[j]}',
                                'INSERT',
                                '{"ID":' || :new_row."ID" || ',"NAME":"' || :new_row."NAME" || '"}'
                            );
                        END;
                        """
                    )
                    // Verify insert trigger creation
                    if (!db.verifyTriggerExists(schemaNames[i], "${tableNames[j]}_before_insert")) {
                        throw SQLException(
                            "Failed to create insert trigger \"${schemaNames[i]}\".\"${tableNames[j]}_before_insert\""
                        )
                    }

                    // Create update trigger
                    db.execute(
                        """
                        CREATE TRIGGER "${schemaNames[i]}"."${tableNames[j]}_before_update"
                        BEFORE UPDATE ON "${schemaNames[i]}"."${tableNames[j]}"
                        REFERENCING NEW ROW new_row, OLD ROW old_row
                        FOR EACH ROW
                        BEGIN
                            INSERT INTO "${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}"."${triggerTableName}" (
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}primary_key",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}stream_name",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}operation_type",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}value_before",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}value_after"
                            )
                            VALUES (
                                :old_row."ID",
                                '${tableNames[j]}',
                                'UPDATE',
                                '{"ID":' || :old_row."ID" || ',"NAME":"' || :old_row."NAME" || '"}',
                                '{"ID":' || :new_row."ID" || ',"NAME":"' || :new_row."NAME" || '"}'
                            );
                        END;
                        """
                    )
                    // Verify update trigger creation
                    if (!db.verifyTriggerExists(schemaNames[i], "${tableNames[j]}_before_update")) {
                        throw SQLException(
                            "Failed to create update trigger \"${schemaNames[i]}\".\"${tableNames[j]}_before_update\""
                        )
                    }

                    // Create delete trigger
                    db.execute(
                        """
                        CREATE TRIGGER "${schemaNames[i]}"."${tableNames[j]}_before_delete"
                        BEFORE DELETE ON "${schemaNames[i]}"."${tableNames[j]}"
                        REFERENCING OLD ROW old_row
                        FOR EACH ROW
                        BEGIN
                            INSERT INTO "${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}"."${triggerTableName}" (
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}primary_key",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}stream_name",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}operation_type",
                                "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}value_before"
                            )
                            VALUES (
                                :old_row."ID",
                                '${tableNames[j]}',
                                'DELETE',
                                '{"ID":' || :old_row."ID" || ',"NAME":"' || :old_row."NAME" || '"}'
                            );
                        END;
                        """
                    )
                    // Verify delete trigger creation
                    if (!db.verifyTriggerExists(schemaNames[i], "${tableNames[j]}_before_delete")) {
                        throw SQLException(
                            "Failed to create delete trigger \"${schemaNames[i]}\".\"${tableNames[j]}_before_delete\""
                        )
                    }

                    // Insert initial data
                    db.execute(
                        "INSERT INTO \"${schemaNames[0]}\".\"${tableNames[0]}\" (\"ID\", \"NAME\") VALUES (10, 'foo')"
                    )
                    db.execute(
                        "INSERT INTO \"${schemaNames[0]}\".\"${tableNames[0]}\" (\"ID\", \"NAME\") VALUES (20, 'bar')"
                    )
                    // Verify data insertion
                    if (!db.verifyDataExists(schemaNames[0], tableNames[0])) {
                        throw SQLException(
                            "Failed to insert data into \"${schemaNames[0]}\".\"${tableNames[0]}\""
                        )
                    }
                }
            }
        } catch (e: SQLException) {
            e.printStackTrace()
            throw e // Re-throw to fail the test
        }
    }

    @AfterEach
    fun tearDown() {
        try {
            for (i in 0..schemaNames.size - 1) {
                for (j in 0..tableNames.size - 1) {
                    val triggerTableName =
                        "${TriggerTableConfig.TRIGGER_TABLE_PREFIX}${schemaNames[i]}_${tableNames[j]}"

                    // Drop triggers
                    db.execute(
                        "DROP TRIGGER \"${schemaNames[i]}\".\"${tableNames[j]}_before_insert\""
                    )
                    db.execute(
                        "DROP TRIGGER \"${schemaNames[i]}\".\"${tableNames[j]}_before_update\""
                    )
                    db.execute(
                        "DROP TRIGGER \"${schemaNames[i]}\".\"${tableNames[j]}_before_delete\""
                    )

                    // Drop trigger table
                    db.execute(
                        "DROP TABLE \"${TriggerTableConfig.TRIGGER_TABLE_NAMESPACE}\".\"${triggerTableName}\""
                    )
                }

                // Drop schema
                db.execute("DROP SCHEMA \"${schemaNames[i]}\" CASCADE")
            }

            // No need to drop the schema for trigger tables

            db.disconnect()
        } catch (e: SQLException) {
            e.printStackTrace()
        }
    }

    @Test
    fun testCheck() {
        val run1: BufferingOutputConsumer = CliRunner.source("check", config, null).run()

        assertEquals(run1.messages().size, 1)
        assertEquals(
            run1.messages().first().connectionStatus.status,
            AirbyteConnectionStatus.Status.SUCCEEDED
        )
    }

    @Test
    fun testCdcRead() {
        val run1: BufferingOutputConsumer =
            CliRunner.source("read", config, getConfiguredCatalog()).run()
        val lastStateMessageFromRun1 = run1.states().last()
        // Verify initial sync captured the existing records
        val recordsFromRun1 = run1.records()
        assertEquals(2, recordsFromRun1.size)
        // Insert a new record to test CDC capture
        db.execute(
            "INSERT INTO \"${schemaNames[0]}\".\"${tableNames[0]}\" (\"ID\", \"NAME\") VALUES (30, 'baz')"
        )

        val run2InputState: List<AirbyteStateMessage> = listOf(lastStateMessageFromRun1)
        val run2: BufferingOutputConsumer =
            CliRunner.source("read", config, getConfiguredCatalog(), run2InputState).run()
        // Verify CDC captured the new record
        val recordsFromRun2 = run2.records()
        // Expecting 2 because we get records == cursor plus new record
        assertEquals(2, recordsFromRun2.size)
        val recordDataRun2 = recordsFromRun2.last().data
        assertEquals("30", recordDataRun2.get("ID").asText())
        // Update a record to test CDC update capture
        db.execute(
            "UPDATE \"${schemaNames[0]}\".\"${tableNames[0]}\" SET \"NAME\" = 'updated' WHERE \"ID\" = 20"
        )
        val lastStateMessageFromRun2 = run2.states().last()
        val run3InputState: List<AirbyteStateMessage> = listOf(lastStateMessageFromRun2)
        val run3: BufferingOutputConsumer =
            CliRunner.source("read", config, getConfiguredCatalog(), run3InputState).run()
        // Verify CDC captured the updated record
        val recordsFromRun3 = run3.records()
        // Expecting 2 because we get records == cursor plus new record
        assertEquals(2, recordsFromRun3.size)
        val recordDataRun3 = recordsFromRun3.last().data
        assertEquals("20", recordDataRun3.get("ID").asText())

        // Delete a record to test CDC delete capture
        db.execute("DELETE FROM \"${schemaNames[0]}\".\"${tableNames[0]}\" WHERE \"ID\" = 10")
        val lastStateMessageFromRun3 = run3.states().last()
        val run4InputState: List<AirbyteStateMessage> = listOf(lastStateMessageFromRun3)
        val run4: BufferingOutputConsumer =
            CliRunner.source("read", config, getConfiguredCatalog(), run4InputState).run()
        // Verify CDC captured the deleted record
        val recordsFromRun4 = run4.records()
        // Expecting 2 because we get records == cursor plus new record
        assertEquals(2, recordsFromRun4.size)

        // Verify the deleted record ID
        val recordDataRun4 = recordsFromRun4.last().data
        assertEquals("10", recordDataRun4.get("ID").asText())
    }

    @Test
    fun testFullRefresh() {
        val fullRefreshCatalog =
            getConfiguredCatalog().apply { streams[0].syncMode = SyncMode.FULL_REFRESH }
        val run1: BufferingOutputConsumer =
            CliRunner.source("read", config, fullRefreshCatalog).run()
        // Verify full refresh captured the existing records
        val recordsFromRun1 = run1.records()
        assertEquals(2, recordsFromRun1.size)
        // Insert a new record
        db.execute(
            "INSERT INTO \"${schemaNames[0]}\".\"${tableNames[0]}\" (\"ID\", \"NAME\") VALUES (30, 'baz')"
        )
        val lastStateMessageFromRun1 = run1.states().last()
        val run2: BufferingOutputConsumer =
            CliRunner.source("read", config, fullRefreshCatalog, listOf(lastStateMessageFromRun1))
                .run()
        // Verify full refresh will not resume if snapshot is completed
        val recordsFromRun2 = run2.records()
        assertEquals(0, recordsFromRun2.size)

        val run3: BufferingOutputConsumer =
            CliRunner.source("read", config, fullRefreshCatalog).run()
        // Verify full refresh captured all records including the new one
        val recordsFromRun3 = run3.records()
        assertEquals(3, recordsFromRun3.size)
    }

    companion object {
        // Using the same test database as in SapHanaSourceCursorBasedIntegrationTest
        var db = SapHanaSourceCursorBasedIntegrationTest.db

        val schemaNames = db.getRandomSchemaNames(1)
        val tableNames = db.getRandomTableNames(1)

        val config: SapHanaSourceConfigurationSpecification =
            SapHanaSourceConfigurationSpecification().apply {
                host = db.host
                port = db.port
                schemas = schemaNames
                username = db.username
                password = db.password
                setCursorMethodValue(
                    CdcCursorConfigurationSpecification().apply {
                        initialLoadTimeoutHours = 8
                        invalidCdcCursorPositionBehavior = "Fail sync"
                    }
                )
            }

        fun getConfiguredCatalog(): ConfiguredAirbyteCatalog {
            val desc = StreamDescriptor().withName(tableNames[0]).withNamespace(schemaNames[0])
            val discoveredStream =
                DiscoveredStream(
                    id = StreamIdentifier.from(desc),
                    columns = listOf(Field("ID", IntFieldType), Field("NAME", StringFieldType)),
                    primaryKeyColumnIDs = listOf(listOf("ID"))
                )
            val stream: AirbyteStream =
                SapHanaSourceOperations()
                    .create(SapHanaSourceConfigurationFactory().make(config), discoveredStream)
            val configuredStream: ConfiguredAirbyteStream =
                CatalogHelpers.toDefaultConfiguredStream(stream)
                    .withSyncMode(SyncMode.INCREMENTAL)
                    .withPrimaryKey(discoveredStream.primaryKeyColumnIDs)
                    .withCursorField(listOf(TriggerTableConfig.CURSOR_FIELD.id))
            return ConfiguredAirbyteCatalog().withStreams(listOf(configuredStream))
        }
    }
}
